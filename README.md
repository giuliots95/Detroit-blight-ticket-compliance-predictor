# Detroit-blight-ticket-compliance-predictor
This repository contains the study on a binary classifier that predicts if a blight ticket issued by Detroit agencies is likely to be paid or not, based on historical data.

What I think is worth of attention, is the application of an evolutionary strategy to perform the hyperparameters research. Instead of using "sklearn" default techniques, like grid search or randomized grid search, I used an adapted version of the genetic algorithm provided in "sklearn-gen-opt" library, to optimize the parameters search. Indeed, the parameters search is normally a time consuming and computationally expensive task, since al lot of models have to be trained and scored, until the "best" hyperparameters set is found. Moreover, for each combination of hyperparameters, the meta-models are trained and validated with the popular 5-fold coss validation (CV) technique. So each meta-model has to be trained and scored 5 times, and obtained performance metric is averaged to get a point estimate of the modelling error associated with the actual hyperparameters configuration. This means that standard grid search or randomized search implemented in "sklearn" library become unreliable, when many hyperparameters combinations have to be tested. 
Instead of trying all hyperparameters combinations in a sequential way, applying an evolutionary strategy (so a genetic algorithm) allows to explore the parameters search space in a more "clever" way: starting from a number of randomly built models, the new ones are chosen trying to maximize the implemented fitness function, in this case the classifier performance metric. Looking at the optimization history logs, one can easily notice that the algorithm "pushes" the hyperparameters configuration towards an improved score in just a few iterations (or using the terminology of the genetic algorithm, in just a few generations).

The original problem requested to build a classifier with a ROC auc score bigger that 0.75 and it was successfully solved with this technique.

Another remark on the implementation of my solution should be done on the imbalanced original dataset. Since the given dataset is highly imbalanced towards the negative class (i.e. fine not paid), I coded a function that conserves the whole minority class, while randomly undersamples the majority class, thus obtaining a fully balanced training set. This could be done in a smoother way calling some methods from "imblearn" library but I wanted to try the simplest undersampling methodology on my own.
Re-balancing the training set turned up to be very useful during model selection phase, but presents a drawback for what concerns the testing phase: testing the best model on a resampled dataset leads to an optimistic estimate of the overall scoring metric. For this reason, before performing the rebalancing task, I sampled a held-out dataset from the original set, in a stratifiyed way. This choice is supposed to produce a held-out set which better represents "real life" data. 
As can be noticed from the latest part of the notebook, the model performance obtained with this held-out testing set is lower, especially for what regards the false positive rates, but still satisfying the ROC AUC goal.

